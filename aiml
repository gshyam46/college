1.	Use Find S Algorithm on weather data and obtain the output
Code:
import numpy as np
import pandas as pd
data=pd.read_csv('Find S algorithm.csv')
def train(concepts,target):
    specific_h=concepts[0]
    for i,h in enumerate (concepts):
        if target[i]=="yes":
            for x in range(len(specific_h)):
                if h[x]==specific_h[x]:
                    pass
                else:
                    specific_h[x]=="?"
    return specific_h
concepts=np.array(data.iloc[:,0:-1])
target=np.array(data.iloc[:,-1])
print('Concept',concepts)
print('Target',target)
print(train(concepts,target))

Output:
Concept [['sunny' 'warm' 'normal' 'strong' 'warm' 'same']
 ['sunny' 'warm' 'high' 'strong' 'warm' 'same']
 ['rainy' 'cold' 'high' 'strong' 'warm' 'change']
 ['sunny' 'warm' 'high' 'strong' 'cool' 'change']]
Target ['yes' 'yes' 'no' 'yes']
['sunny' 'warm' 'normal' 'strong' 'warm' 'same']


2.	Use KNN Algorithm on iris dataset and obtain the desired output
Code:
from sklearn.neighbors import KNeighborsClassifier 
from sklearn.model_selection import train_test_split 
from sklearn import datasets
iris=datasets.load_iris() 
print("Iris Data set loaded...")
X_train, X_test, y_train, y_test = train_test_split(iris.data,iris.target,test_size=0.1)
print("Dataset is split into training and testing...")
print("Size of training data and its label",X_train.shape,y_train.shape)
print("Size of test data and its label",X_test.shape,y_test.shape)
for i in range(len(iris.target_names)):
    print("label",i,"-",str(iris.target_names[i]))
classifier = KNeighborsClassifier(n_neighbors=1) 
classifier.fit(X_train, y_train)  
y_pred=classifier.predict(X_test)
print("Results of classification using KNN with k=1")
for r in range(0,len(X_test)):
    print("Sample: ",str(X_test[r]),"Actual label: ",str(y_test[r]),"Predicted label: ",str(y_pred[r]))
print("Classification Accuracy: ",classifier.score(X_test,y_test))
from sklearn.metrics import classification_report,confusion_matrix
print('Confusion Matrix: ')
print(confusion_matrix(y_test,y_pred))
print('Accuracy Metrics: ')
print(classification_report(y_test,y_pred))

Output:
Iris Data set loaded...
Dataset is split into training and testing...
Size of training data and its label (135, 4) (135,)
Size of test data and its label (15, 4) (15,)
label 0 - setosa
label 1 - versicolor
label 2 - virginica
Results of classification using KNN with k=1
Sample:  [5.2 4.1 1.5 0.1] Actual label:  0 Predicted label:  0
Sample:  [6.5 3.  5.5 1.8] Actual label:  2 Predicted label:  2
Sample:  [5.2 3.5 1.5 0.2] Actual label:  0 Predicted label:  0
Sample:  [7.1 3.  5.9 2.1] Actual label:  2 Predicted label:  2
Sample:  [5.2 2.7 3.9 1.4] Actual label:  1 Predicted label:  1
Sample:  [6.3 2.3 4.4 1.3] Actual label:  1 Predicted label:  1
Sample:  [6.4 3.2 4.5 1.5] Actual label:  1 Predicted label:  1
Sample:  [6.9 3.1 5.4 2.1] Actual label:  2 Predicted label:  2
Sample:  [6.  2.2 4.  1. ] Actual label:  1 Predicted label:  1
Sample:  [6.4 3.1 5.5 1.8] Actual label:  2 Predicted label:  2
Sample:  [6.6 2.9 4.6 1.3] Actual label:  1 Predicted label:  1
Sample:  [5.5 3.5 1.3 0.2] Actual label:  0 Predicted label:  0
Sample:  [6.7 2.5 5.8 1.8] Actual label:  2 Predicted label:  2
Sample:  [7.  3.2 4.7 1.4] Actual label:  1 Predicted label:  1
Sample:  [5.5 2.3 4.  1.3] Actual label:  1 Predicted label:  1
Classification Accuracy:  1.0
Confusion Matrix: 
[[3 0 0]
 [0 7 0]
 [0 0 5]]
Accuracy Metrics: 
              precision    recall  f1-score   support

           0       1.00      1.00      1.00         3
           1       1.00      1.00      1.00         7
           2       1.00      1.00      1.00         5

    accuracy                           1.00        15
   macro avg       1.00      1.00      1.00        15
weighted avg       1.00      1.00      1.00        15


3.	Use PCA on breast cancer data and display the output
Code:
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
%matplotlib inline
from sklearn.datasets import load_breast_cancer
cancer=load_breast_cancer()
cancer.keys()
print(cancer['feature_names'])
for i in range(len(cancer.target_names)):
    print("Label",i,"-",str(cancer.target_names[i]))
print(cancer['DESCR'])
df=pd.DataFrame(cancer['data'],columns=cancer['feature_names'])
df.head(5)
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
scaler.fit(df)
scaled_data=scaler.transform(df)
scaled_data
from sklearn.decomposition import PCA
pca=PCA(n_components=2)
pca.fit(scaled_data)
x_pca=pca.transform(scaled_data)
scaled_data.shape
x_pca.shape
scaled_data
x_pca
plt.figure(figsize=(8,6))
plt.scatter(x_pca[:,0],x_pca[:,1],c=cancer['target'])
plt.xlabel('First principle component')
plt.ylabel('Second principle component')

Output:
['mean radius' 'mean texture' 'mean perimeter' 'mean area'
 'mean smoothness' 'mean compactness' 'mean concavity'
 'mean concave points' 'mean symmetry' 'mean fractal dimension'
 'radius error' 'texture error' 'perimeter error' 'area error'
 'smoothness error' 'compactness error' 'concavity error'
 'concave points error' 'symmetry error' 'fractal dimension error'
 'worst radius' 'worst texture' 'worst perimeter' 'worst area'
 'worst smoothness' 'worst compactness' 'worst concavity'
 'worst concave points' 'worst symmetry' 'worst fractal dimension']
Label 0 - malignant
Label 1 - benign
.. _breast_cancer_dataset:

Breast cancer wisconsin (diagnostic) dataset
--------------------------------------------

**Data Set Characteristics:**

    :Number of Instances: 569

    :Number of Attributes: 30 numeric, predictive attributes and the class

    :Attribute Information:
        - radius (mean of distances from center to points on the perimeter)
        - texture (standard deviation of gray-scale values)
        - perimeter
        - area
        - smoothness (local variation in radius lengths)
        - compactness (perimeter^2 / area - 1.0)
        - concavity (severity of concave portions of the contour)
        - concave points (number of concave portions of the contour)
        - symmetry
        - fractal dimension ("coastline approximation" - 1)

        The mean, standard error, and "worst" or largest (mean of the three
        worst/largest values) of these features were computed for each image,
        resulting in 30 features.  For instance, field 0 is Mean Radius, field
        10 is Radius SE, field 20 is Worst Radius.

        - class:
                - WDBC-Malignant
                - WDBC-Benign

    :Summary Statistics:

    ===================================== ====== ======
                                           Min    Max
    ===================================== ====== ======
    radius (mean):                        6.981  28.11
    texture (mean):                       9.71   39.28
    perimeter (mean):                     43.79  188.5
    area (mean):                          143.5  2501.0
    smoothness (mean):                    0.053  0.163
    compactness (mean):                   0.019  0.345
    concavity (mean):                     0.0    0.427
    concave points (mean):                0.0    0.201
    symmetry (mean):                      0.106  0.304
    fractal dimension (mean):             0.05   0.097
    radius (standard error):              0.112  2.873
    texture (standard error):             0.36   4.885
    perimeter (standard error):           0.757  21.98
    area (standard error):                6.802  542.2
    smoothness (standard error):          0.002  0.031
    compactness (standard error):         0.002  0.135
    concavity (standard error):           0.0    0.396
    concave points (standard error):      0.0    0.053
    symmetry (standard error):            0.008  0.079
    fractal dimension (standard error):   0.001  0.03
    radius (worst):                       7.93   36.04
    texture (worst):                      12.02  49.54
    perimeter (worst):                    50.41  251.2
    area (worst):                         185.2  4254.0
    smoothness (worst):                   0.071  0.223
    compactness (worst):                  0.027  1.058
    concavity (worst):                    0.0    1.252
    concave points (worst):               0.0    0.291
    symmetry (worst):                     0.156  0.664
    fractal dimension (worst):            0.055  0.208
    ===================================== ====== ======

    :Missing Attribute Values: None

    :Class Distribution: 212 - Malignant, 357 - Benign

    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian

    :Donor: Nick Street

    :Date: November, 1995

This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.
https://goo.gl/U2Uwz2

Features are computed from a digitized image of a fine needle
aspirate (FNA) of a breast mass.  They describe
characteristics of the cell nuclei present in the image.

Separating plane described above was obtained using
Multisurface Method-Tree (MSM-T) [K. P. Bennett, "Decision Tree
Construction Via Linear Programming." Proceedings of the 4th
Midwest Artificial Intelligence and Cognitive Science Society,
pp. 97-101, 1992], a classification method which uses linear
programming to construct a decision tree.  Relevant features
were selected using an exhaustive search in the space of 1-4
features and 1-3 separating planes.

The actual linear program used to obtain the separating plane
in the 3-dimensional space is that described in:
[K. P. Bennett and O. L. Mangasarian: "Robust Linear
Programming Discrimination of Two Linearly Inseparable Sets",
Optimization Methods and Software 1, 1992, 23-34].

This database is also available through the UW CS ftp server:

ftp ftp.cs.wisc.edu
cd math-prog/cpo-dataset/machine-learn/WDBC/

.. topic:: References

   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction 
     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on 
     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,
     San Jose, CA, 1993.
   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and 
     prognosis via linear programming. Operations Research, 43(4), pages 570-577, 
     July-August 1995.
   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques
     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) 
     163-171.
[9]:
Text(0, 0.5, 'Second principle component')

[ ]:


4.	Use Linear regression model on randomly generated data and obtain the output
Code:
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
#Generate some synthetic data for demonstration
np.random.seed(0)
X=2*np.random.rand(100,1)
y=4+3*X+np.random.randn(100,1)
#split the data into training and testing sets
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)
#train the linear regression model
lin_reg=LinearRegression()
lin_reg.fit(X_train,y_train)
#make predictions on the test set
y_pred=lin_reg.predict(X_test)
#calculate Mean Squared Error (MSE) as a measure of performance
mse=mean_squared_error(y_test,y_pred)
print("Mean Squared error: ",mse)
#plot the training data, the fitted line, and the test data
plt.scatter(X_train,y_train,color='blue',label='Training Data')
plt.scatter(X_test,y_test,color='green', label='Test Data')
plt.plot(X_test,y_pred, color='red',linewidth=2,label='Fitted Line')
plt.xlabel('X')
plt.ylabel('Y')
plt.title('Linear Regression')
plt.legend()
plt.show()

Output:
Mean Squared error:  0.9177532469714291
\


5.	Use Linear regression model on randomly generated data and obtain the output
Code:
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
#Import dataset
dataset=pd.read_csv('Linear Regression.csv')
X=dataset.iloc[:,:-1].values
y=dataset.iloc[:,1].values
#split the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=1/3,random_state=0)
#fitting linear regression to training set
from sklearn.linear_model import LinearRegression
regressor=LinearRegression()
regressor.fit(X_train,y_train)
#make predictions on the test set
y_pred=regressor.predict(X_test)
#plot the training data, the fitted line, and the test data
plt.scatter(X_train,y_train,color='red',label='Training Data')
plt.plot(X_train,regressor.predict(X_train), color='blue')
plt.scatter(X_test,y_test,color='yellow')
plt.plot(X_train,regressor.predict(X_train), color='blue')
plt.xlabel('Years')
plt.ylabel('Salary')
plt.title('Salary vs Years')
plt.show()

Output:




6.	Write a python program using Logistic regression on iris dataset

Code:
import numpy as np
import matplotlib.pyplot as plt
from sklearn import  datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
iris=datasets.load_iris()
X=iris.data
y=iris.target
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=0)
scaler=StandardScaler()
X_train=scaler.fit_transform(X_train)
X_test=scaler.transform(X_test)
model=LogisticRegression()
model.fit(X_train,y_train)
y_pred=model.predict(X_test)
from sklearn.metrics import confusion_matrix
cm=confusion_matrix(y_test,y_pred)
print("Confusion matrix: ")
print(cm)
accuracy=accuracy_score(y_test,y_pred)
print("Accuracy: ",accuracy)

Output:
Confusion matrix: 
[[16  0  0]
 [ 0 17  1]
 [ 0  0 11]]
Accuracy:  0.9777777777777777


7.	Write a python code using Naïve Bayes algorithm on iris dataset and display the accuracy and confusion matrix

Code:
from sklearn.naive_bayes import GaussianNB
from sklearn.datasets import load_iris
from sklearn import metrics
from sklearn.model_selection import train_test_split as split
dataset=load_iris()
x=dataset.data
y=dataset.target
x_train,x_test,y_train,y_test=split(x,y,test_size=0.2,random_state=1)
gnb=GaussianNB()
classifier=gnb.fit(x_train,y_train)
y_pred=classifier.predict(x_test)
print("Accuracy Metrics :")
print(metrics.classification_report(y_test,y_pred))
print("The accuracy of metrics is: ",metrics.accuracy_score(y_test,y_pred))
print("Confusion matrix: ")
print(metrics.confusion_matrix(y_test,y_pred))

Output:
Accuracy Metrics :
              precision    recall  f1-score   support

           0       1.00      1.00      1.00        11
           1       1.00      0.92      0.96        13
           2       0.86      1.00      0.92         6

    accuracy                           0.97        30
   macro avg       0.95      0.97      0.96        30
weighted avg       0.97      0.97      0.97        30

The accuracy of metrics is:  0.9666666666666667
Confusion matrix
[[11  0  0]
 [ 0 12  1]
 [ 0  0  6]]

8.	Write a Naïve Bayes program on analysing documents in python

Code:
import pandas as pd
msg=pd.read_csv('Naive Bayes.csv',names=['message','label'])
print("Total instances in the datset: ",msg.shape[0])
msg['labelnum']=msg.label.map({'pos':1,'neg':0})
x=msg.message
y=msg.labelnum
from sklearn.model_selection import train_test_split
xtrain,xtest,ytrain,ytest=train_test_split(x,y)
print('\nDataset is split into Training and Testing samples')
print('Total training instances: ',xtrain.shape[0])
print('Total testing instances: ',xtest.shape[0])
from sklearn.feature_extraction.text import CountVectorizer
count_vect=CountVectorizer()
xtrain_dtm=count_vect.fit_transform(xtrain)
xtest_dtm=count_vect.transform(xtest)
print('\nTotal features extracted using CountVectorizer: ',xtrain_dtm.shape[1])
print('\nFeatures for first 5 training instances are listed below')
df=pd.DataFrame(xtrain_dtm.toarray(),columns=count_vect.get_feature_names_out())
print(df[0:5])
from sklearn.naive_bayes import MultinomialNB
clf=MultinomialNB().fit(xtrain_dtm,ytrain)
predicted=clf.predict(xtest_dtm)
print('\nClassification results of testing samples are given below')
for doc,p in zip(xtest,predicted):
    pred='pos' if p==1 else 'neg'
    print('%s -> %s'%(doc,pred))
from sklearn import metrics
print('\nAccuracy metrics')
print('Accuracy of the classifier is',metrics.accuracy_score(ytest,predicted))
print('Recall: ',metrics.recall_score(ytest,predicted),'\nPrecision: ',metrics.precision_score(ytest,predicted))
print('Confusion matrix')
print(metrics.confusion_matrix(ytest,predicted))

Output:
Total instances in the datset:  18

Dataset is split into Training and Testing samples
Total training instances:  13
Total testing instances:  5

Total features extracted using CountVectorizer:  40

Features for first 5 training instances are listed below
   am  amazing  an  awesome  bad  best  boss  can  dance  deal  ...  the  \
0   0        0   0        0    0     0     1    0      0     0  ...    0   
1   0        0   0        0    0     0     0    1      0     1  ...    0   
2   1        0   0        0    0     0     0    0      0     0  ...    0   
3   0        0   0        0    0     0     0    0      0     0  ...    0   
4   0        0   1        1    0     0     0    0      0     0  ...    0   

   this  tired  to  today  view  went  what  with  work  
0     0      0   0      0     0     0     0     0     0  
1     1      0   0      0     0     0     0     1     0  
2     1      1   0      0     0     0     0     0     0  
3     1      0   0      0     0     0     0     0     0  
4     1      0   0      0     0     0     0     0     0  

[5 rows x 40 columns]

Classification results of testing samples are given below
I feel very good about these beers -> neg
I am sick and tired of this place -> neg
We will have good fun tomorrow -> neg
What a great holiday -> pos
I do not like this restaurant -> neg

Accuracy metrics
Accuracy of the classifier is 0.6
Recall:  0.3333333333333333 
Precision:  1.0
Confusion matrix
[[2 0]
 [2 1]]

9.	Write a program for AND operations using perceptron

Code:
def activation(out,threshold):
    if out>=threshold:
        return 1
    else:
        return 0
def perceptron(and_input):
    a=[0,0,1,1]
    b=[0,1,0,1]
    y=[0,0,0,1]
    w=[1.4,1.5]
    threshold=1
    learning_rate=0.1
    i=0
    print("Perceptron Training : ")
    print('######################')
    print('------------')
    while i<4:
        summation = a[i]*w[0] + b[i]*w[1]
        o= activation(summation,threshold)
        print("Input : " + str(a[i]) + " , "+ str(b[i]))
        print("Weights: " +str(w[0]) + " , " + str(w[1])) 
        print("summation : "+str(summation) + "threshold : "+str(threshold))
        print("Actual Output : "+str(y[i])+" Predicted Output : "+str(o))
        if(o!=y[i]):
            print("____________\nUpdating weights")
            w[0]=w[0]+learning_rate*(y[i]-o)*a[i]
            print("w[o]",w[o])
            w[1]=w[1]+learning_rate*(y[i]-o)*b[i]
            print("Updated Weights : " + str(w[0]) + " , "+ str(w[1]))
            i=-1
            print("\nWeights Updated Training Again : ")
            print("###################################")
        i=i+1
        print("---------------")
    summation = and_input[0]*w[0] + and_input[1]*w[1]
    return activation(summation,threshold)

and_input = [1,1]
print("AND Gate Output for " +str(and_input) + " : "+str(perceptron(and_input)))

Output:

Perceptron Training : 
######################
------------
Input : 0 , 0
Weights: 1.4 , 1.5
summation : 0.0threshold : 1
Actual Output : 0 Predicted Output : 0
---------------
Input : 0 , 1
Weights: 1.4 , 1.5
summation : 1.5threshold : 1
Actual Output : 0 Predicted Output : 1
____________
Updating weights
w[o] 1.5
Updated Weights : 1.4 , 1.4

Weights Updated Training Again : 
###################################
---------------
Input : 0 , 0
Weights: 1.4 , 1.4
summation : 0.0threshold : 1
Actual Output : 0 Predicted Output : 0
---------------
Input : 0 , 1
Weights: 1.4 , 1.4
summation : 1.4threshold : 1
Actual Output : 0 Predicted Output : 1
____________
Updating weights
w[o] 1.4
Updated Weights : 1.4 , 1.2999999999999998

Weights Updated Training Again : 
###################################
---------------
Input : 0 , 0
Weights: 1.4 , 1.2999999999999998
summation : 0.0threshold : 1
Actual Output : 0 Predicted Output : 0
---------------
Input : 0 , 1
Weights: 1.4 , 1.2999999999999998
summation : 1.2999999999999998threshold : 1
Actual Output : 0 Predicted Output : 1
____________
Updating weights
w[o] 1.2999999999999998
Updated Weights : 1.4 , 1.1999999999999997

Weights Updated Training Again : 
###################################
---------------
Input : 0 , 0
Weights: 1.4 , 1.1999999999999997
summation : 0.0threshold : 1
Actual Output : 0 Predicted Output : 0
---------------
Input : 0 , 1
Weights: 1.4 , 1.1999999999999997
summation : 1.1999999999999997threshold : 1
Actual Output : 0 Predicted Output : 1
____________
Updating weights
w[o] 1.1999999999999997
Updated Weights : 1.4 , 1.0999999999999996

Weights Updated Training Again : 
###################################
---------------
Input : 0 , 0
Weights: 1.4 , 1.0999999999999996
summation : 0.0threshold : 1
Actual Output : 0 Predicted Output : 0
---------------
Input : 0 , 1
Weights: 1.4 , 1.0999999999999996
summation : 1.0999999999999996threshold : 1
Actual Output : 0 Predicted Output : 1
____________
Updating weights
w[o] 1.0999999999999996
Updated Weights : 1.4 , 0.9999999999999997

Weights Updated Training Again : 
###################################
---------------
Input : 0 , 0
Weights: 1.4 , 0.9999999999999997
summation : 0.0threshold : 1
Actual Output : 0 Predicted Output : 0
---------------
Input : 0 , 1
Weights: 1.4 , 0.9999999999999997
summation : 0.9999999999999997threshold : 1
Actual Output : 0 Predicted Output : 0
---------------
Input : 1 , 0
Weights: 1.4 , 0.9999999999999997
summation : 1.4threshold : 1
Actual Output : 0 Predicted Output : 1
____________
Updating weights
w[o] 0.9999999999999997
Updated Weights : 1.2999999999999998 , 0.9999999999999997

Weights Updated Training Again : 
###################################
---------------
Input : 0 , 0
Weights: 1.2999999999999998 , 0.9999999999999997
summation : 0.0threshold : 1
Actual Output : 0 Predicted Output : 0
---------------
Input : 0 , 1
Weights: 1.2999999999999998 , 0.9999999999999997
summation : 0.9999999999999997threshold : 1
Actual Output : 0 Predicted Output : 0
---------------
Input : 1 , 0
Weights: 1.2999999999999998 , 0.9999999999999997
summation : 1.2999999999999998threshold : 1
Actual Output : 0 Predicted Output : 1
____________
Updating weights
w[o] 0.9999999999999997
Updated Weights : 1.1999999999999997 , 0.9999999999999997

Weights Updated Training Again : 
###################################
---------------
Input : 0 , 0
Weights: 1.1999999999999997 , 0.9999999999999997
summation : 0.0threshold : 1
Actual Output : 0 Predicted Output : 0
---------------
Input : 0 , 1
Weights: 1.1999999999999997 , 0.9999999999999997
summation : 0.9999999999999997threshold : 1
Actual Output : 0 Predicted Output : 0
---------------
Input : 1 , 0
Weights: 1.1999999999999997 , 0.9999999999999997
summation : 1.1999999999999997threshold : 1
Actual Output : 0 Predicted Output : 1
____________
Updating weights
w[o] 0.9999999999999997
Updated Weights : 1.0999999999999996 , 0.9999999999999997

Weights Updated Training Again : 
###################################
---------------
Input : 0 , 0
Weights: 1.0999999999999996 , 0.9999999999999997
summation : 0.0threshold : 1
Actual Output : 0 Predicted Output : 0
---------------
Input : 0 , 1
Weights: 1.0999999999999996 , 0.9999999999999997
summation : 0.9999999999999997threshold : 1
Actual Output : 0 Predicted Output : 0
---------------
Input : 1 , 0
Weights: 1.0999999999999996 , 0.9999999999999997
summation : 1.0999999999999996threshold : 1
Actual Output : 0 Predicted Output : 1
____________
Updating weights
w[o] 0.9999999999999997
Updated Weights : 0.9999999999999997 , 0.9999999999999997

Weights Updated Training Again : 
###################################
---------------
Input : 0 , 0
Weights: 0.9999999999999997 , 0.9999999999999997
summation : 0.0threshold : 1
Actual Output : 0 Predicted Output : 0
---------------
Input : 0 , 1
Weights: 0.9999999999999997 , 0.9999999999999997
summation : 0.9999999999999997threshold : 1
Actual Output : 0 Predicted Output : 0
---------------
Input : 1 , 0
Weights: 0.9999999999999997 , 0.9999999999999997
summation : 0.999x9999999999997threshold : 1
Actual Output : 0 Predicted Output : 0
---------------
Input : 1 , 1
Weights: 0.9999999999999997 , 0.9999999999999997
summation : 1.9999999999999993threshold : 1
Actual Output : 1 Predicted Output : 1
---------------
AND Gate Output for [1, 1] : 1

10.	Write a program for OR gate using perceptron

Code:
def activation(out,threshold):
    if out>=threshold:
        return 1
    else:
        return 0
def perceptron(or_input):
    a=[0,0,1,1]
    b=[0,1,0,1]
    y=[0,1,1,1]
    w=[0.0,0.3]
    threshold=0.4
    learning_rate=0.5
    i=0
    print("Perceptron Training : ")
    print('######################')
    print('------------')
    while i<4:
        summation = a[i]*w[0] + b[i]*w[1]
        o= activation(summation,threshold)
        print("Input : " + str(a[i]) + " , " + str(b[i]))
        print("Weights: " +str(w[0]) + " , " + str(w[1])) 
        print("summation : "+str(summation) + "threshold : "+str(threshold))
        print("Actual Output : "+str(y[i])+" Predicted Output : "+str(o))
        if(o!=y[i]):
            print("____________\nUpdating weights")
            w[0]=w[0]+learning_rate*(y[i]-o)*a[i]
            print("w[o]",w[o])
            w[1]=w[1]+learning_rate*(y[i]-o)*b[i]
            print("Updated Weights : " + str(w[0]) + " , "+ str(w[1]))
            i=-1
            print("\nWeights Updated Training Again : ")
            print("###################################")
        i=i+1
        print("---------------")
    summation = or_input[0]*w[0] + or_input[1]*w[1]
    return activation(summation,threshold)

or_input = [1,1]
print("OR Gate Output for " +str(or_input) + " : "+str(perceptron(or_input)))

Output:
Perceptron Training : 
######################
------------
Input : 0 , 0
Weights: 0.0 , 0.3
summation : 0.0threshold : 0.4
Actual Output : 0 Predicted Output : 0
---------------
Input : 0 , 1
Weights: 0.0 , 0.3
summation : 0.3threshold : 0.4
Actual Output : 1 Predicted Output : 0
____________
Updating weights
w[o] 0.0
Updated Weights : 0.0 , 0.8

Weights Updated Training Again : 
###################################
---------------
Input : 0 , 0
Weights: 0.0 , 0.8
summation : 0.0threshold : 0.4
Actual Output : 0 Predicted Output : 0
---------------
Input : 0 , 1
Weights: 0.0 , 0.8
summation : 0.8threshold : 0.4
Actual Output : 1 Predicted Output : 1
---------------
Input : 1 , 0
Weights: 0.0 , 0.8
summation : 0.0threshold : 0.4
Actual Output : 1 Predicted Output : 0
____________
Updating weights
w[o] 0.5
Updated Weights : 0.5 , 0.8

Weights Updated Training Again : 
###################################
---------------
Input : 0 , 0
Weights: 0.5 , 0.8
summation : 0.0threshold : 0.4
Actual Output : 0 Predicted Output : 0
---------------
Input : 0 , 1
Weights: 0.5 , 0.8
summation : 0.8threshold : 0.4
Actual Output : 1 Predicted Output : 1
---------------
Input : 1 , 0
Weights: 0.5 , 0.8
summation : 0.5threshold : 0.4
Actual Output : 1 Predicted Output : 1
---------------
Input : 1 , 1
Weights: 0.5 , 0.8
summation : 1.3threshold : 0.4
Actual Output : 1 Predicted Output : 1
---------------
OR Gate Output for [1, 1] : 1

11.	Analyze the working of perceptron and error function using suitable dataset

Code:
import numpy as np
def activation_function(x):
    return 1 if x >= 0 else 0
def predict(inputs, weights, bias):
    weighted_sum = np. dot(inputs, weights) + bias
    return activation_function (weighted_sum)
def train_perceptron(training_inputs, labels, learning_rate=0.01, num_epochs=100):
    num_inputs = len(training_inputs[0])
    weights = np. random. rand (num_inputs)
    bias = np. random.rand()
    for epoch in range(num_epochs):
        for inputs, label in zip(training_inputs, labels):
            prediction = predict(inputs, weights, bias)
            error = label - prediction
            weights += learning_rate * error * inputs 
            bias += learning_rate * error
    return weights, bias
if __name__=="__main__":
    training_inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
    labels = np.array([0, 0, 0, 1])
    weights, bias = train_perceptron(training_inputs, labels)
    test_inputs = np.array ([[0, 0], [0, 1], [1, 0], [1, 1]])
    for inputs in test_inputs:
       print(f"Input: {inputs}, Predicted Output: {predict(inputs, weights, bias)}")

Output:
Input: [0 0], Predicted Output: 0
Input: [0 1], Predicted Output: 0
Input: [1 0], Predicted Output: 0
Input: [1 1], Predicted Output: 1

12.	Construct Multilayer perceptron

Code:
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
data = pd.read_csv("Perceptron.csv")
X = data.drop('Class', axis=1)
y = data['Class']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
model = Sequential()
model.add(Dense(32, input_dim=X_train.shape[1], activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(32, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.summary()
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.1)
loss, accuracy = model.evaluate(X_test, y_test)
print(f'Test Loss: {loss:.4f}')
print(f'Test Accuracy: {accuracy:.4f}')

Output:
Test Loss: 0.0050
Test Accuracy: 0.9994

13.	Build an artificial neural network by implementing the backpropagation       algorithm and test the same using appropriate datasets

Code:
import numpy as np
X=np.array(([2,9],[1,5],[3,6]),dtype=float)
y=np.array(([92],[86],[89]),dtype=float)
X=X/np.amax(X,axis=0)
y=y/100
def sigmoid(x):
    return 1/(1+np.exp(-x))
def derivatives_sigmoid(x):
    return x*(1-x)
epoch=7000
lr=0.1
inputlayer_neurons=2
hiddenlayer_neurons=3
output_neurons=1
wh=np.random.uniform(size=(inputlayer_neurons,hiddenlayer_neurons))
bh=np.random.uniform(size=(1,hiddenlayer_neurons))
wout=np.random.uniform(size=(hiddenlayer_neurons,output_neurons))
bout=np.random.uniform(size=(1,output_neurons))
for i in range(epoch):
    hinpl=np.dot(X,wh)
    hinp=hinpl+bh
    hlayer_act=sigmoid(hinp)
    outinpl=np.dot(hlayer_act,wout)
    outinp=outinpl+bout
    output=sigmoid(outinp)
    EO=y-output
    outgrad=derivatives_sigmoid(output)
    d_output=(EO*outgrad)
    EH=d_output.dot(wout.T)
    hiddengrad=derivatives_sigmoid(hlayer_act)
    d_hiddenlayer=EH*hiddengrad
    wout+=hlayer_act.T.dot(d_output)*lr
    wh+=X.T.dot(d_hiddenlayer)*lr
print("Input: \n"+str(X))
print("Atual Output:\n"+str(y))
print("Predicted Output:\n",output)

Output:
Input: 
[[0.66666667 1.        ]
 [0.33333333 0.55555556]
 [1.         0.66666667]]
Atual Output:
[[0.92]
 [0.86]
 [0.89]]
Predicted Output:
 [[0.89304735]
 [0.88208296]
 [0.89538126]]

14.	Apply SVD algorithm on random array

Code:
import numpy as np
A = np.random.rand(5, 3)
U, S, VT = np.linalg.svd(A)
k = 2
U_k = U[:, :k]
S_k = np.diag(S[:k])
VT_k = VT[:k, :]
A_reduced = np.dot(U_k, np.dot(S_k, VT_k))
print("Original matrix:\n", A)
print("Reduced matrix:\n", A_reduced)

Ouptut:
Original matrix:
 [[0.26667962 0.12131455 0.64817706]
 [0.92163017 0.8814406  0.31165481]
 [0.36442665 0.74439496 0.27971447]
 [0.27139962 0.0794924  0.54717   ]
 [0.95478142 0.66983474 0.22132221]]
Reduced matrix:
 [[0.27391583 0.11448034 0.64632081]
 [0.9169252  0.88588419 0.31286174]
 [0.57403474 0.54643154 0.22594543]
 [0.24281504 0.10648898 0.55450257]
 [0.81922741 0.79785812 0.25609477]]

15.	Apply SVD algorithm on random data and display the respective details

Code:
from skimage.color import rgb2gray
from skimage import data
import matplotlib.pyplot as plt
import numpy as np
from scipy.linalg import svd
X = np.array([[3, 3, 2], [2, 3, -2]])
print(X)
U, singular, V_transpose = svd(X)
print("U: ", U)
print("Singular array", singular)
print("V^{T}", V_transpose)

Output:
[[ 3  3  2]
 [ 2  3 -2]]
U:  [[ 0.7815437 -0.6238505]
 [ 0.6238505  0.7815437]]
Singular array [5.54801894 2.86696457]
V^{T} [[ 0.64749817  0.7599438   0.05684667]
 [-0.10759258  0.16501062 -0.9804057 ]
 [-0.75443354  0.62869461  0.18860838]]

16.	Use LDA algorithm Iris data set and display the accuracy and confusion
matrix

Code:
import numpy as np
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.metrics import accuracy_score, confusion_matrix
iris = load_iris()
X = iris.data
y = iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
lda = LinearDiscriminantAnalysis()
lda.fit(X_train, y_train)
y_pred = lda.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
print("Accuracy:", accuracy)
print("Confusion Matrix:\n", conf_matrix)

Output:
Accuracy: 1.0
Confusion Matrix:
 [[19  0  0]
 [ 0 13  0]
 [ 0  0 13]]

17.	Use LDA algorithm on iris dataset and display the data in a graph

Code:
import numpy as np
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt
iris = load_iris()
X = iris.data
y = iris.target
lda = LDA(n_components=2)
X_lda = lda.fit_transform(X, y)
plt.scatter(X_lda[:, 0], X_lda[:, 1], c=y)
plt.xlabel('Linear Discriminant 1')
plt.ylabel('Linear Discriminant 2')
plt.title('LDA of Iris Dataset')
plt.show()



Output:


